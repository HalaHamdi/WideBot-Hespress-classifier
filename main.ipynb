{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from Feature_extraction import *\n",
    "from classical_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test  = pd.read_csv('test.csv')\n",
    "encode_category(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['story'] =train['story'] .apply(preprocess_arabic_text)\n",
    "test['story'] =test['story'] .apply(preprocess_arabic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test=TFIDF(train['story'],test['story'])\n",
    "y_train=train['topic']\n",
    "y_test=test['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch(x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Metrics:\n",
      "Precision (weighted): 0.836\n",
      "Recall (weighted): 0.832\n",
      "F1 Score (weighted): 0.833\n",
      "Accuracy: 0.832\n",
      "\n",
      "Metrics for Each Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.94      0.87       200\n",
      "           1       0.82      0.85      0.84       200\n",
      "           2       0.94      0.93      0.93       200\n",
      "           3       0.83      0.88      0.85       200\n",
      "           4       0.96      0.84      0.90       200\n",
      "           5       0.57      0.57      0.57       200\n",
      "           6       0.73      0.80      0.76       200\n",
      "           7       0.85      0.84      0.85       200\n",
      "           8       0.72      0.65      0.68       200\n",
      "           9       0.99      0.93      0.96       200\n",
      "          10       0.97      0.92      0.95       200\n",
      "\n",
      "    accuracy                           0.83      2200\n",
      "   macro avg       0.84      0.83      0.83      2200\n",
      "weighted avg       0.84      0.83      0.83      2200\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8362110634463815,\n",
       " 0.8322727272727273,\n",
       " 0.8328458816783101,\n",
       " 0.8322727272727273)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=svm_model(x_train,y_train,x_test)\n",
    "get_metrics(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">|Model |Paramerters |Precision| Recall|F1 SCORE |  Accuracy|\n",
    ">|----|----|----|----|----|----|\n",
    ">|SVM|    kernel=rbf  |  0.836|   0.832| 0.833 |0.832|\n",
    ">|SVM|    kernel=linear  |  0.826|   0.826|  0.825 | 0.826|\n",
    ">|Linear Regression|    - |0.830 | 0.829|   0.827|  0.829|\n",
    ">|XGboost|   - |  0.799|  0.801|0.798 |0.801|\n",
    ">|Descision Tree|    - |0.638  |  0.639|  0.637|  0.639|\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### _`precision:`_  $  \\frac  {tp}{tp+fp} $\n",
    "> it is more concerned with how many of the classified topics are the trie ones without falsely classifying some into the wrong class \n",
    ">\n",
    "> #### _`recall:`_  $  \\frac  {tp}{tp+fn}$ \n",
    "> it is more concerned with how many of the true topics have i correclty noted as related to that class of topic\n",
    ">\n",
    "> #### _`f-score:`_ $  \\frac  {precision *  recall}{precision +  recall}$\n",
    "> it is a good mertic to use in case of imbalance in class , which is not our case here because the classes are balanced\n",
    ">\n",
    "> #### _`accuracy:`_ $  \\frac  {tp+tn}{tp+fp+fn+tn}$\n",
    "> since we have balanced class , it refers how many correct topics have i classified, so i correctly noticed that it is lying in topic1 and noticed that it doesnot lie in topic2\n",
    "\n",
    "\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
